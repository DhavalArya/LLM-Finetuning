{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "RpqvpMdaSJgj",
   "metadata": {
    "id": "RpqvpMdaSJgj"
   },
   "source": [
    "# QLoRA Training Notebook (Judge Model)\n",
    "\n",
    "This trains a lightweight **scoring/judge** model using **QLoRA** on your `sft.jsonl` labels.\n",
    "\n",
    "### What you need to do before running\n",
    "1. **Colab Runtime → GPU** (T4 is OK; A100 is faster).\n",
    "2. **Accept the model license** on Hugging Face (e.g. `meta-llama/Llama-3.2-1B-Instruct`).\n",
    "3. Obtain a **Hugging Face access token** with read access and be ready to paste it when prompted.\n",
    "4. Upload your data files to Colab:\n",
    "   - `/content/data/sft.jsonl` (train)\n",
    "   - `/content/data/val.jsonl` (optional val)\n",
    "\n",
    "### Outputs\n",
    "- Trained LoRA adapter at `/content/qlora-judge-ckpt/` (zipped for download)\n",
    "- (Optional) quick validation generation on a small subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "I73U1dYVVZCk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I73U1dYVVZCk",
    "outputId": "c5b91dc0-e052-4bad-ab39-f8e1cc651b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 25.6 MB/s eta 0:00:00\n",
      "NumPy 1.26.4 SciPy 1.14.1\n",
      "Torch 2.3.1+cu121 CUDA 12.1\n",
      "bnb file: /usr/local/lib/python3.12/dist-packages/bitsandbytes/__init__.py\n",
      "bnb libs: ['/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda125_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda124_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda125.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda124.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123.so']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
      "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
      "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
      "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
      "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
      "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
      "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
      "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
      "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
      "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
      "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
      "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "# Be explicit & quiet\n",
    "python -V\n",
    "pip -q install --upgrade pip\n",
    "\n",
    "# Remove conflicting preinstalls (ok if some aren't present)\n",
    "pip -q uninstall -y numpy scipy numba opencv-python opencv-contrib-python opencv-python-headless || true\n",
    "\n",
    "# Pin versions that satisfy Colab deps (opencv wants numpy>=2,<2.3; numba<2.1; gcsfs wants newer fsspec)\n",
    "pip -q install \"numpy==2.0.2\" \"scipy==1.14.1\" \"fsspec==2025.3.0\"\n",
    "\n",
    "# Torch + CUDA 12.1 wheels\n",
    "pip -q install --upgrade --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "\n",
    "# bitsandbytes GPU build + triton providing triton.ops\n",
    "pip -q install bitsandbytes==0.43.3 triton==2.3.0\n",
    "\n",
    "# HF training stack\n",
    "pip -q install transformers==4.45.0 accelerate==0.34.2 peft==0.11.1 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "# Sanity print\n",
    "python - << 'PY'\n",
    "import numpy, scipy, torch, bitsandbytes as bnb, triton, glob\n",
    "print(\"NumPy\", numpy.__version__, \"SciPy\", scipy.__version__)\n",
    "print(\"Torch\", torch.__version__, \"CUDA\", torch.version.cuda)\n",
    "print(\"bnb file:\", bnb.__file__)\n",
    "print(\"bnb libs:\", glob.glob(\"/usr/local/lib/python*/dist-packages/bitsandbytes/libbitsandbytes_cuda*.so\"))\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ySiBrDs8Xi",
   "metadata": {
    "id": "b7ySiBrDs8Xi"
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade \"transformers>=4.45.0\" \"accelerate>=0.33.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ijnkfgb3SJgm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijnkfgb3SJgm",
    "outputId": "8e3fbb7c-7105-4f5b-9dc4-e65484c9e483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda  dtype: torch.float16\n",
      "If you have not accepted the model license on HF, please do that now before logging in.\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, math, gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "major_cc = torch.cuda.get_device_capability(0)[0] if torch.cuda.is_available() else 0\n",
    "DTYPE = torch.bfloat16 if (DEVICE=='cuda' and major_cc>=8) else torch.float16  # bf16 on A100/L4; fp16 on T4\n",
    "print('Device:', DEVICE, ' dtype:', DTYPE)\n",
    "\n",
    "# Choose a small instruction-tuned base. Accept license on HF first.\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"  # you can switch to Qwen/Qwen2.5-1.5B-Instruct if preferred\n",
    "DATA_TRAIN = \"/content/data/sft.jsonl\"\n",
    "DATA_VAL   = \"/content/data/val.jsonl\"  # optional\n",
    "OUTPUT_DIR = \"/content/qlora-judge-ckpt\"\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print('If you have not accepted the model license on HF, please do that now before logging in.')\n",
    "login(token=\"<REPLACE_WITH_YOUR_HF_TOKEN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feP27hGUSJgm",
   "metadata": {
    "id": "feP27hGUSJgm"
   },
   "source": [
    "## Load dataset (SFT JSONL)\n",
    "Expected JSONL per line:\n",
    "```json\n",
    "{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}\n",
    "```\n",
    "The model learns to map `(instruction + input) → output`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VVcQFF3NSJgm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "VVcQFF3NSJgm",
    "outputId": "c418da17-3750-4a36-eed4-c6fd62d8cbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 1407  Val examples: 156\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'You are a scoring model. Output STRICT JSON only — no extra text.\\n\\n### Instruction:\\nGiven a business JSON and the executive summary text, produce STRICT JSON with fields:\\n{\\n  \"sections\": { <section>: { \"answer_relevancy\":0..1, \"hallucination\":0..1, \"summarization\":0..1, \"toxicity\":0..1, \"bias\":0..1 }, ... },\\n  \"overall\":  { \"answer_relevancy\":0..1, \"hallucination\":0..1, \"summarization\":0..1, \"toxicity\":0..1, \"bias\":0..1 }\\n}\\nOnly include sections that are present in the business JSON (e.g., no ma'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "train_rows = load_jsonl(DATA_TRAIN)\n",
    "val_rows = load_jsonl(DATA_VAL) if os.path.exists(DATA_VAL) else []\n",
    "print('Train examples:', len(train_rows), ' Val examples:', len(val_rows))\n",
    "\n",
    "def format_example(o):\n",
    "    intro = \"You are a scoring model. Output STRICT JSON only — no extra text.\"\n",
    "    return {\n",
    "        'text': intro + \"\\n\\n### Instruction:\\n\" + o['instruction'] + \"\\n\\n### Input:\\n\" + o['input'] + \"\\n\\n### Response:\\n\" + o['output']\n",
    "    }\n",
    "\n",
    "train_ds = Dataset.from_list([format_example(x) for x in train_rows])\n",
    "val_ds = Dataset.from_list([format_example(x) for x in val_rows]) if val_rows else None\n",
    "train_ds[0]['text'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j_wF_8HySJgm",
   "metadata": {
    "id": "j_wF_8HySJgm"
   },
   "source": [
    "## Load tokenizer & 4-bit base (BitsAndBytes nf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fu5Q0TstSJgm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382,
     "referenced_widgets": [
      "f4137b1653514d9d8671f3194b100bb9",
      "be785fe5e6744bce865fcd173e851792",
      "4723b54018cf4359b53db61f86d28de9",
      "355ae98df3f7484a9eef5e5e26da928a",
      "65aeb156735748c5b27aff8016f9c383",
      "031701b56edc40bfba063dea80b66650",
      "6c06c183d33544b29458c59499fa97bf",
      "e6b73c42d4ab4f2d9544870ced260f50",
      "511b07084c814655ae7ef034e2dd2eb0",
      "31f527f343984efba8059bf2043bb5b5",
      "7b15491ac0474248ad7997e55d6759a8",
      "a8f1165c24f74af68a10aefc9125f256",
      "6f52676c2e24447db20f7114ddbf5937",
      "081750f528644145afebf4d466090a56",
      "b713b781c35b474ab50b0a9a5c13ca73",
      "25a0b7662fd4416fa2d3730a4195b92d",
      "7e1f8d941fdd4e93817e1a39f9ecd931",
      "5b7f9e3697a0494b87b237061d676b6e",
      "17a1c1c958184a51b16f1dee33e44510",
      "49bd47327e004fa28cf475280e524927",
      "55a0774097434fbab988bbf4191b9341",
      "c79d5eae878748b4ba5c29a92c331f5e",
      "7996406cf33f4a8b8c2baae8a47451d7",
      "dadd85b147ce4d1589ca27688482fe0f",
      "7c06d2cae9694cb283dd77546e0f6963",
      "8c26f8683d0149d581dc4028263b7ae9",
      "8e8fb420712a4e339a8df9c506dd2487",
      "bd548aea192640c98490ae4073784926",
      "c9e99ea2d06f4626b0b33759f4a754f5",
      "0829f32c155146119b03f83f6bf76ba9",
      "b96d920ae135409cbe9c53321a41787a",
      "6b1368fad2d44cc3b50b8375d2b74451",
      "3053d65043d645368234caa95fb30af9",
      "659f728fa74e4eb0a4af6f1210e808dc",
      "97aba34d94524359838be7bac215ca92",
      "fe711ef4b2eb49e7a59b931a105862b0",
      "cb68af3406144fa3af485f4d65144981",
      "d97200e8c0cb4bbd8af595e487a3a51a",
      "eeafedfd57a04be3afaaa4ccc5ba092b",
      "13d1cb07b26a49ffb50f3f870698b3bc",
      "0ae87ec4c223457182e6ae277966557a",
      "e23430ad2eaa421f9d54a04695627214",
      "a5c9478a7d8b468aa081a2cae01c3ca1",
      "ef58ae040af34abb88239952877f97d9",
      "1e17312c254d42df9a3009d34cbe6fe1",
      "29852955aaf5429c8afce141dae3b195",
      "aa887100df02464fac4c5a175dfd4362",
      "e4741ada3d2c4863914e8192b01c9cac",
      "4e94aa08497f417fbe4e9b3568175eeb",
      "76fc68abb5a04b7a9bcaba7f47c449fc",
      "634b5a5c2b1a4f8a8849d257293d03e3",
      "d6c6803f76224f27bc70f58fdefcfa96",
      "2d36a64f42c54cc8b0783863390d459e",
      "b0fd020c193145689a2441d47a512c5a",
      "91a9e3e8a3a9488a82d1de58c0df6daa",
      "2f7df8adc36747d89f9fd629da1bdc89",
      "7dcc5d632fd34f4fac059f7c70657fd2",
      "0a592ace569a4594981800852cf2c6ec",
      "4071a900c36049c7bf8b774529026875",
      "5d21ef9308584c6b9818151e4418408c",
      "44daaabcbbf843628f1837d193899e9a",
      "b1cb654427a346679fe3cef606e375f6",
      "cbaecdaae9e6477f96676a1377fbecfe",
      "ba388eb65f2d48e2a67f88ffd65b04d2",
      "1366de860e5043679be914bf4df9b86b",
      "235287502bfe4f03b98d1d8f36eff9cb",
      "7ec26f2d66bd4934ba3e21e4d4e6f57a",
      "9ff5393848ea4291a1806fad41e008a8",
      "1759d146ad7e48dd988e5ef10aaac526",
      "e28a0ee91f024aa4bdf07b51823cf880",
      "45060ac30b994731a1e7eff5ee9e172d",
      "4426cbb66b584703aa98524d8f854e35",
      "e044170d8f6d4a5ab0fbf1d9415b966f",
      "3237cc42acb2421bbadbac9d582ecf43",
      "075b513c4a684704b55d1079cbd3fc4e",
      "01c94daf36a34314bbd6f1a27cdafce6",
      "9dc18af6649f49258ca46079eeb3883f"
     ]
    },
    "id": "fu5Q0TstSJgm",
    "outputId": "37060a13-acf4-4fe8-b4b2-b763bf76dc03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4137b1653514d9d8671f3194b100bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f1165c24f74af68a10aefc9125f256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7996406cf33f4a8b8c2baae8a47451d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659f728fa74e4eb0a4af6f1210e808dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e17312c254d42df9a3009d34cbe6fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7df8adc36747d89f9fd629da1bdc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec26f2d66bd4934ba3e21e4d4e6f57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # or your model with access granted\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=DTYPE,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=DTYPE,\n",
    "    attn_implementation=\"sdpa\",   # safe default\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-qdBGj6dSJgm",
   "metadata": {
    "id": "-qdBGj6dSJgm"
   },
   "source": [
    "## PEFT LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "EF9DNSHiSJgm",
   "metadata": {
    "id": "EF9DNSHiSJgm"
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, get_peft_model\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias='none',\n",
    "#     task_type='CAUSAL_LM',\n",
    "#     target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voHQNm6_SJgn",
   "metadata": {
    "id": "voHQNm6_SJgn"
   },
   "source": [
    "## Train with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1EASfUbvRlh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1EASfUbvRlh",
    "outputId": "cb29c27c-55e2-4bbb-c11a-34c139b916a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# must be False when using gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# enable grad flow with ckpt\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# cast norms, set grad ckpt hooks, etc.\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",   # attention\n",
    "    \"gate_proj\",\"up_proj\",\"down_proj\"      # mlp\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # <-- should show non-zero trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tMtnZfvZwzpm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "tMtnZfvZwzpm",
    "outputId": "43cde8e8-eb36-454a-cf75-dee36547de03"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-575189175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time; time.sleep(24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "-m7TZSV4SJgn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320,
     "referenced_widgets": [
      "d0660b5e50c749959004d62b4f0c3c19",
      "97d4ab2e78ee47469f215fd702679458",
      "c96bbf33aa2a48dc8da99e0a5e4c87ac",
      "bd5d4444399d41b0adca6bbcd3cdda42",
      "5050c2df72094d9b9db79aa76a8cd01c",
      "68481bd769e54af4a4cef53b40dd0573",
      "57a8fa4718f3417ca981b59bb25ed506",
      "35f4a99cf0fc4b938ecc4aaa823ce310",
      "fa60e9752b4c42a3b5f2cd8eddb11374",
      "b696d4a3a98349d8b871db926e302914",
      "a736f90111a643648d9a7582cc16fc98",
      "d4bd8b3437144551a884604b7540de89",
      "e33d2d88bd6045768d75c165d267a3b5",
      "bd64ecacdd6b44d08b8b8b437cade489",
      "7b2f63ab926c4d958d531dc27b8ed17c",
      "26771b817c0c40728061b8b3ba9a93aa",
      "8a222896b57e4cbfb7a809013e27cadc",
      "eca605c2171643b0b2f34cf2182b8350",
      "afe769d758894ebf97846923b51fe04f",
      "f03ae22e993246ad8fe950e2400fd46b",
      "46d0686947224a54a4ea22aa9b47da22",
      "d8a0257132f4441390c00ea2de714f95"
     ]
    },
    "id": "-m7TZSV4SJgn",
    "outputId": "08fad475-eac7-44d1-b6e7-9fbd37439e9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0660b5e50c749959004d62b4f0c3c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd8b3437144551a884604b7540de89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_packed rows: 1408 val_packed rows: 156\n",
      "input_ids <class 'torch.Tensor'> torch.Size([2, 1024])\n",
      "labels <class 'torch.Tensor'> torch.Size([2, 1024])\n",
      "attention_mask <class 'torch.Tensor'> torch.Size([2, 1024])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 1:30:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.647500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: /content/drive/MyDrive/qlora-judge\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import DefaultDataCollator, TrainingArguments, Trainer\n",
    "\n",
    "MAX_SEQ_LEN = 2048\n",
    "EPOCHS      = 1\n",
    "GRAD_ACCUM  = 12\n",
    "OVERLAP     = 0           # set to 128 later if time still < 3h\n",
    "\n",
    "BATCH  = 1\n",
    "LR     = 1e-4\n",
    "DTYPE  = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# tokenizer safety\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side    = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "\n",
    "# tokenize WITHOUT truncation; we will split docs into chunks\n",
    "def tok_rows_no_trunc(ds):\n",
    "    return ds.map(\n",
    "        lambda b: tokenizer(\n",
    "            b[\"text\"],\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_attention_mask=False\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=\"Tokenizing (no truncation)\"\n",
    "    )\n",
    "\n",
    "train_tok = tok_rows_no_trunc(train_ds)\n",
    "val_tok   = tok_rows_no_trunc(val_ds)\n",
    "\n",
    "# slice each doc into fixed-size blocks (optional overlap)\n",
    "def chunk_per_example(tok_ds, seq_len, eos_id, min_tail=16, overlap=0):\n",
    "    rows = []\n",
    "    step = max(1, seq_len - overlap)\n",
    "    for ex in tok_ds:\n",
    "        ids = ex.get(\"input_ids\", [])\n",
    "        if not ids:\n",
    "            continue\n",
    "        ids = ids + [eos_id]     # boundary marker\n",
    "        L = len(ids)\n",
    "        i = 0\n",
    "        while i < L:\n",
    "            block = ids[i:i+seq_len]\n",
    "            if len(block) < min_tail:\n",
    "                break\n",
    "            rows.append({\n",
    "                \"input_ids\": block,\n",
    "                \"labels\": block,\n",
    "                \"attention_mask\": [1]*len(block)\n",
    "            })\n",
    "            i += step\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_packed = chunk_per_example(train_tok, MAX_SEQ_LEN, tokenizer.eos_token_id, overlap=OVERLAP)\n",
    "val_packed   = chunk_per_example(val_tok,   MAX_SEQ_LEN, tokenizer.eos_token_id, overlap=OVERLAP)\n",
    "print(\"train_packed rows:\", len(train_packed), \"val_packed rows:\", len(val_packed))\n",
    "\n",
    "collator = DefaultDataCollator()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    bf16=(DTYPE==torch.bfloat16),\n",
    "    fp16=(DTYPE==torch.float16),\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,      # ON so 2048 fits on T4\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_packed,\n",
    "    eval_dataset=val_packed,          # will run at end of each epoch\n",
    ")\n",
    "\n",
    "# quick batch sanity\n",
    "batch = collator([train_packed[i] for i in range(min(2, len(train_packed)))])\n",
    "for k,v in batch.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"✅ Saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_isMl6DcSJgn",
   "metadata": {
    "id": "_isMl6DcSJgn"
   },
   "source": [
    "## Zip for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ydIYgR1SJgn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ydIYgR1SJgn",
    "outputId": "80be5483-03d7-4f6e-edf2-cd4e3fb164bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: qlora-judge-ckpt/ (stored 0%)\n",
      "-rw-r--r-- 1 root root 184 Oct 16 07:17 /content/qlora-judge-ckpt.zip\n"
     ]
    }
   ],
   "source": [
    "!cd /content && zip -r qlora-judge-ckpt.zip qlora-judge-ckpt\n",
    "!ls -lah /content/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8WGb-P0pSJgn",
   "metadata": {
    "id": "8WGb-P0pSJgn"
   },
   "source": [
    "### Notes & Precautions\n",
    "- Keep the tab active. Save checkpoints to Drive if the run is long.\n",
    "- If OOM: reduce `MAX_SEQ_LEN`, use smaller `BATCH`, increase `GRAD_ACCUM`.\n",
    "- Your output JSON must be **STRICT**; include a few format-enforcer samples if needed.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
