{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW6UN02tRpaS",
        "outputId": "ae36b279-464a-4cbf-84f0-b317d241867a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣  Clean up possible broken preinstalls\n",
        "!pip uninstall -y bitsandbytes triton transformers peft accelerate xformers\n",
        "\n",
        "# 2️⃣  Install correct CUDA-enabled bitsandbytes build + dependencies\n",
        "!pip install -q bitsandbytes==0.43.3\n",
        "!pip install -q triton==2.3.0\n",
        "!pip install -q torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 3️⃣  Install Hugging Face stack\n",
        "!pip install -q transformers==4.44.2 peft==0.10.0 accelerate==0.34.2 datasets==2.21.0 scipy numpy\n",
        "\n",
        "# 4️⃣  (Optional but helpful) install xformers for faster attention\n",
        "!pip install -q xformers==0.0.27\n",
        "\n",
        "# 5️⃣  Environment variable to reduce fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# 6️⃣  Quick sanity checks\n",
        "import torch, bitsandbytes as bnb, triton, transformers, peft, accelerate\n",
        "print(\"✅ Torch:\", torch.__version__)\n",
        "print(\"✅ CUDA available:\", torch.cuda.is_available())\n",
        "print(\"✅ bitsandbytes path:\", bnb.__file__)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81O4hEymYsNB",
        "outputId": "86a4513b-ae4f-4bf4-aee8-bb7810c4b246"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: triton 3.4.0\n",
            "Uninstalling triton-3.4.0:\n",
            "  Successfully uninstalled triton-3.4.0\n",
            "Found existing installation: transformers 4.57.0\n",
            "Uninstalling transformers-4.57.0:\n",
            "  Successfully uninstalled transformers-4.57.0\n",
            "Found existing installation: peft 0.17.1\n",
            "Uninstalling peft-0.17.1:\n",
            "  Successfully uninstalled peft-0.17.1\n",
            "Found existing installation: accelerate 1.10.1\n",
            "Uninstalling accelerate-1.10.1:\n",
            "  Successfully uninstalled accelerate-1.10.1\n",
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "torch 2.8.0+cu126 requires triton==3.4.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m828.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m787.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Torch: 2.3.1+cu121\n",
            "✅ CUDA available: True\n",
            "✅ bitsandbytes path: /usr/local/lib/python3.12/dist-packages/bitsandbytes/__init__.py\n",
            "Thu Oct 16 14:33:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ADAPTER_PATH = \"/content/qlora-judge-ckpt\"\n",
        "DATA_PATH = \"/content/val.jsonl\"\n",
        "\n",
        "OUT_CSV = \"/content/judge_eval.csv\"\n",
        "OUT_REPORT = \"/content/judge_metrics.json\""
      ],
      "metadata": {
        "id": "RpGgdAlVSELM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re, csv, os, torch, numpy as np\n",
        "from typing import Dict, Any, List, Optional\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# =========================\n",
        "# ✅ CONFIG\n",
        "# =========================\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ADAPTER_PATH = \"/content/qlora-judge-ckpt\"   # your LoRA adapter path\n",
        "DATA_PATH = \"/content/val.jsonl\"        # evaluation dataset path\n",
        "OUT_CSV = \"/content/judge_eval.csv\"\n",
        "OUT_REPORT = \"/content/judge_metrics.json\"\n",
        "MAX_LEN = 2048\n",
        "MAX_NEW_TOKENS = 64\n",
        "METRICS = [\"answer_relevancy\", \"hallucination\", \"summarization\", \"toxicity\", \"bias\"]\n",
        "\n",
        "# =========================\n",
        "# ✅ Helper functions\n",
        "# =========================\n",
        "def load_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except Exception: continue\n",
        "    return rows\n",
        "\n",
        "def safe_json(txt: str):\n",
        "    \"\"\"Extract and parse JSON object from raw model text.\"\"\"\n",
        "    m = re.search(r\"\\{.*\\}\", txt, re.S)\n",
        "    if not m: return None\n",
        "    try:\n",
        "        return json.loads(m.group(0))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_overall(js: Dict[str, Any]) -> Optional[Dict[str, float]]:\n",
        "    \"\"\"Extract top-level or 'overall' metric scores.\"\"\"\n",
        "    if not isinstance(js, dict): return None\n",
        "    ov = js.get(\"overall\") or js.get(\"scores\") or js\n",
        "    res = {}\n",
        "    for k in METRICS:\n",
        "        try:\n",
        "            if ov.get(k) is not None:\n",
        "                res[k] = float(ov[k])\n",
        "        except Exception:\n",
        "            continue\n",
        "    return res or None\n",
        "\n",
        "def build_prompt(instr: str, inp: str) -> str:\n",
        "    return f\"{instr.strip()}\\n\\n{inp.strip()}\\n\"\n",
        "\n",
        "def chunk_text(tokenizer, text, max_len=2048):\n",
        "    \"\"\"Split long text into <=max_len chunks (preserves all input).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    for i in range(0, len(tokens), max_len):\n",
        "        yield tokenizer.decode(tokens[i:i + max_len], skip_special_tokens=True)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_json(model, tokenizer, prompt: str, max_new_tokens=64):\n",
        "    \"\"\"Generate JSON output for one prompt chunk.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
        "                       max_length=MAX_LEN, padding=False).to(model.device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    comp = text[len(prompt):].strip()\n",
        "    return safe_json(comp)\n",
        "\n",
        "def average_json_scores(json_list):\n",
        "    \"\"\"Average multiple JSONs (for multi-chunk prompts).\"\"\"\n",
        "    agg = {k: [] for k in METRICS}\n",
        "    for js in json_list:\n",
        "        sc = extract_overall(js)\n",
        "        if sc:\n",
        "            for k in METRICS:\n",
        "                if k in sc:\n",
        "                    agg[k].append(sc[k])\n",
        "    return {k: float(np.mean(v)) for k, v in agg.items() if v}\n",
        "\n",
        "def correlations(true, pred):\n",
        "    if len(true) < 3: return None, None\n",
        "    try:\n",
        "        return pearsonr(true, pred)[0], spearmanr(true, pred)[0]\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "# =========================\n",
        "# ✅ Model Loading\n",
        "# =========================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "print(\"🔹 Loading base model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=dtype, device_map=\"auto\")\n",
        "base_model.eval()\n",
        "\n",
        "print(\"🔹 Loading tuned model (base + adapter)...\")\n",
        "tuned = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=dtype, device_map=\"auto\")\n",
        "tuned = PeftModel.from_pretrained(tuned, ADAPTER_PATH)\n",
        "tuned.eval()\n",
        "\n",
        "# =========================\n",
        "# ✅ Evaluation loop\n",
        "# =========================\n",
        "rows = load_jsonl(DATA_PATH)\n",
        "csv_rows = []\n",
        "gold_vec, base_vec, tuned_vec = {m: [] for m in METRICS}, {m: [] for m in METRICS}, {m: [] for m in METRICS}\n",
        "\n",
        "for i, ex in enumerate(rows, 1):\n",
        "    instr, inp, gold_raw = ex.get(\"instruction\", \"\"), ex.get(\"input\", \"\"), ex.get(\"output\", \"\")\n",
        "    gold = json.loads(gold_raw) if isinstance(gold_raw, str) else gold_raw\n",
        "    gold_sc = extract_overall(gold)\n",
        "    if not gold_sc:\n",
        "        continue\n",
        "\n",
        "    prompt = build_prompt(instr, inp)\n",
        "    # detect if prompt exceeds context limit\n",
        "    tok_len = len(tokenizer.encode(prompt))\n",
        "    if tok_len > MAX_LEN:\n",
        "        print(f\"⚠️ Prompt {i} too long ({tok_len}), splitting...\")\n",
        "        chunks = list(chunk_text(tokenizer, prompt, MAX_LEN))\n",
        "    else:\n",
        "        chunks = [prompt]\n",
        "\n",
        "    # generate for each chunk & average\n",
        "    base_preds, tuned_preds = [], []\n",
        "    for chunk in chunks:\n",
        "        base_pred = generate_json(base_model, tokenizer, chunk, MAX_NEW_TOKENS)\n",
        "        tuned_pred = generate_json(tuned, tokenizer, chunk, MAX_NEW_TOKENS)\n",
        "        if base_pred: base_preds.append(base_pred)\n",
        "        if tuned_pred: tuned_preds.append(tuned_pred)\n",
        "\n",
        "    base_sc = average_json_scores(base_preds)\n",
        "    tuned_sc = average_json_scores(tuned_preds)\n",
        "\n",
        "    row = {\"id\": i}\n",
        "    for m in METRICS:\n",
        "        g = gold_sc.get(m)\n",
        "        b = base_sc.get(m) if base_sc else None\n",
        "        t = tuned_sc.get(m) if tuned_sc else None\n",
        "        row[f\"gold.{m}\"] = g\n",
        "        row[f\"base.{m}\"] = b\n",
        "        row[f\"tuned.{m}\"] = t\n",
        "        if g is not None and b is not None and t is not None:\n",
        "            gold_vec[m].append(g)\n",
        "            base_vec[m].append(b)\n",
        "            tuned_vec[m].append(t)\n",
        "    csv_rows.append(row)\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Processed {i}/{len(rows)} examples...\")\n",
        "\n",
        "# =========================\n",
        "# ✅ Metrics aggregation\n",
        "# =========================\n",
        "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=sorted(csv_rows[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(csv_rows)\n",
        "\n",
        "report = {}\n",
        "for m in METRICS:\n",
        "    g, b, t = np.array(gold_vec[m]), np.array(base_vec[m]), np.array(tuned_vec[m])\n",
        "    if len(g) == 0:\n",
        "        continue\n",
        "    mse_b, mae_b = float(np.mean((g - b)**2)), float(np.mean(np.abs(g - b)))\n",
        "    mse_t, mae_t = float(np.mean((g - t)**2)), float(np.mean(np.abs(g - t)))\n",
        "    pear_b, spear_b = correlations(g.tolist(), b.tolist())\n",
        "    pear_t, spear_t = correlations(g.tolist(), t.tolist())\n",
        "    report[m] = {\n",
        "        \"base\": {\"mse\": mse_b, \"mae\": mae_b, \"pearson\": pear_b, \"spearman\": spear_b},\n",
        "        \"tuned\": {\"mse\": mse_t, \"mae\": mae_t, \"pearson\": pear_t, \"spearman\": spear_t},\n",
        "        \"count\": len(g)\n",
        "    }\n",
        "\n",
        "with open(OUT_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(\"\\n✅ Evaluation complete.\")\n",
        "print(json.dumps(report, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ryMh1ISXMq",
        "outputId": "93d41055-8217-43de-91cc-5ff5b0978b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Loading base model...\n",
            "🔹 Loading tuned model (base + adapter)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6560 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Prompt 1 too long (6560), splitting...\n",
            "⚠️ Prompt 2 too long (5453), splitting...\n",
            "⚠️ Prompt 3 too long (6738), splitting...\n",
            "⚠️ Prompt 4 too long (6561), splitting...\n",
            "⚠️ Prompt 5 too long (6641), splitting...\n",
            "Processed 5/156 examples...\n",
            "⚠️ Prompt 6 too long (6467), splitting...\n",
            "⚠️ Prompt 7 too long (5569), splitting...\n",
            "⚠️ Prompt 8 too long (6623), splitting...\n",
            "⚠️ Prompt 9 too long (5477), splitting...\n",
            "⚠️ Prompt 10 too long (6587), splitting...\n",
            "Processed 10/156 examples...\n",
            "⚠️ Prompt 11 too long (6639), splitting...\n",
            "⚠️ Prompt 12 too long (6607), splitting...\n",
            "⚠️ Prompt 13 too long (6539), splitting...\n",
            "⚠️ Prompt 14 too long (6498), splitting...\n",
            "⚠️ Prompt 15 too long (6582), splitting...\n",
            "Processed 15/156 examples...\n",
            "⚠️ Prompt 16 too long (6574), splitting...\n",
            "⚠️ Prompt 17 too long (6616), splitting...\n",
            "⚠️ Prompt 18 too long (5224), splitting...\n",
            "⚠️ Prompt 19 too long (6553), splitting...\n",
            "⚠️ Prompt 20 too long (6570), splitting...\n",
            "Processed 20/156 examples...\n",
            "⚠️ Prompt 21 too long (6567), splitting...\n",
            "⚠️ Prompt 22 too long (6536), splitting...\n",
            "⚠️ Prompt 23 too long (6611), splitting...\n",
            "⚠️ Prompt 24 too long (6420), splitting...\n",
            "⚠️ Prompt 25 too long (5021), splitting...\n",
            "Processed 25/156 examples...\n",
            "⚠️ Prompt 26 too long (6542), splitting...\n",
            "⚠️ Prompt 27 too long (5545), splitting...\n",
            "⚠️ Prompt 28 too long (6605), splitting...\n",
            "⚠️ Prompt 29 too long (6420), splitting...\n",
            "⚠️ Prompt 30 too long (6552), splitting...\n",
            "Processed 30/156 examples...\n",
            "⚠️ Prompt 31 too long (6602), splitting...\n",
            "⚠️ Prompt 32 too long (5182), splitting...\n",
            "⚠️ Prompt 33 too long (6447), splitting...\n",
            "⚠️ Prompt 34 too long (6710), splitting...\n",
            "⚠️ Prompt 35 too long (6602), splitting...\n",
            "Processed 35/156 examples...\n",
            "⚠️ Prompt 36 too long (6586), splitting...\n",
            "⚠️ Prompt 37 too long (6579), splitting...\n",
            "⚠️ Prompt 38 too long (5239), splitting...\n",
            "⚠️ Prompt 39 too long (6551), splitting...\n",
            "⚠️ Prompt 40 too long (6603), splitting...\n",
            "Processed 40/156 examples...\n",
            "⚠️ Prompt 41 too long (6550), splitting...\n",
            "⚠️ Prompt 42 too long (6604), splitting...\n",
            "⚠️ Prompt 43 too long (6440), splitting...\n",
            "⚠️ Prompt 44 too long (6656), splitting...\n",
            "⚠️ Prompt 45 too long (6577), splitting...\n",
            "Processed 45/156 examples...\n",
            "⚠️ Prompt 46 too long (6625), splitting...\n",
            "⚠️ Prompt 47 too long (6537), splitting...\n",
            "⚠️ Prompt 48 too long (5662), splitting...\n",
            "⚠️ Prompt 49 too long (6512), splitting...\n",
            "⚠️ Prompt 50 too long (6631), splitting...\n",
            "Processed 50/156 examples...\n",
            "⚠️ Prompt 51 too long (6471), splitting...\n",
            "⚠️ Prompt 52 too long (5234), splitting...\n",
            "⚠️ Prompt 53 too long (6598), splitting...\n",
            "⚠️ Prompt 54 too long (6649), splitting...\n",
            "⚠️ Prompt 55 too long (6620), splitting...\n",
            "Processed 55/156 examples...\n",
            "⚠️ Prompt 56 too long (6552), splitting...\n",
            "⚠️ Prompt 57 too long (6390), splitting...\n",
            "⚠️ Prompt 58 too long (5494), splitting...\n",
            "⚠️ Prompt 59 too long (6554), splitting...\n",
            "⚠️ Prompt 60 too long (4184), splitting...\n",
            "Processed 60/156 examples...\n",
            "⚠️ Prompt 61 too long (6563), splitting...\n",
            "⚠️ Prompt 62 too long (6561), splitting...\n",
            "⚠️ Prompt 63 too long (6641), splitting...\n",
            "⚠️ Prompt 64 too long (6566), splitting...\n",
            "⚠️ Prompt 65 too long (6395), splitting...\n",
            "Processed 65/156 examples...\n",
            "⚠️ Prompt 66 too long (5263), splitting...\n",
            "⚠️ Prompt 67 too long (5361), splitting...\n",
            "⚠️ Prompt 68 too long (6570), splitting...\n",
            "⚠️ Prompt 69 too long (6527), splitting...\n",
            "⚠️ Prompt 70 too long (6536), splitting...\n",
            "Processed 70/156 examples...\n",
            "⚠️ Prompt 71 too long (6555), splitting...\n",
            "⚠️ Prompt 72 too long (5468), splitting...\n",
            "⚠️ Prompt 73 too long (6621), splitting...\n",
            "⚠️ Prompt 74 too long (6633), splitting...\n",
            "⚠️ Prompt 75 too long (5150), splitting...\n",
            "Processed 75/156 examples...\n",
            "⚠️ Prompt 76 too long (5544), splitting...\n",
            "⚠️ Prompt 77 too long (6549), splitting...\n",
            "⚠️ Prompt 78 too long (5270), splitting...\n",
            "⚠️ Prompt 79 too long (6548), splitting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRINuJxNTDWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}